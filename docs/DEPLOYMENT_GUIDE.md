# Qwen3-0.6B æ ‘è“æ´¾éƒ¨ç½²å®Œæ•´æŒ‡å—

## ç›®å½•
1. [ç¡¬ä»¶éœ€æ±‚](#1-ç¡¬ä»¶éœ€æ±‚)
2. [è½¯ä»¶ç¯å¢ƒå‡†å¤‡](#2-è½¯ä»¶ç¯å¢ƒå‡†å¤‡)
3. [æ¨¡å‹è½¬æ¢ä¸ä¼ è¾“](#3-æ¨¡å‹è½¬æ¢ä¸ä¼ è¾“)
4. [éƒ¨ç½²æ–¹æ¡ˆå¯¹æ¯”](#4-éƒ¨ç½²æ–¹æ¡ˆå¯¹æ¯”)
5. [æ–¹æ¡ˆA: llama.cpp éƒ¨ç½²](#5-æ–¹æ¡ˆa-llamacpp-éƒ¨ç½²)
6. [æ–¹æ¡ˆB: Ollama éƒ¨ç½²](#6-æ–¹æ¡ˆb-ollama-éƒ¨ç½²)
7. [æ€§èƒ½ä¼˜åŒ–](#7-æ€§èƒ½ä¼˜åŒ–)
8. [å¸¸è§é—®é¢˜](#8-å¸¸è§é—®é¢˜)

---

## 1. ç¡¬ä»¶éœ€æ±‚

### 1.1 å¿…éœ€è®¾å¤‡æ¸…å•

| è®¾å¤‡ | è§„æ ¼ | ä»·æ ¼ï¼ˆUSDï¼‰ | è´­ä¹°é“¾æ¥ | è¯´æ˜ |
|------|------|------------|---------|------|
| **æ ‘è“æ´¾ 5** | 8GB RAM ç‰ˆæœ¬ | $80 | [å®˜æ–¹åº—](https://www.raspberrypi.com/products/raspberry-pi-5/) | **å¼ºçƒˆæ¨è** 8GB ç‰ˆæœ¬ |
| **ç”µæºé€‚é…å™¨** | 27W USB-C PD | $12 | æ ‘è“æ´¾å®˜æ–¹ | å¿…é¡»ä½¿ç”¨å®˜æ–¹æˆ–è®¤è¯ç”µæº |
| **MicroSD å¡** | 128GB Class 10/A2 | $15 | Amazon / äº¬ä¸œ | æ¨è SanDisk Extreme |
| **æ•£çƒ­å™¨** | ä¸»åŠ¨æ•£çƒ­é£æ‰‡ | $8 | æ ‘è“æ´¾å®˜æ–¹ | é•¿æ—¶é—´è¿è¡Œå¿…å¤‡ |
| **æœºç®±** | å®˜æ–¹æœºç®± | $10 | æ ‘è“æ´¾å®˜æ–¹ | å¯é€‰ï¼Œä½†æ¨è |

**æ€»æˆæœ¬**: ~$125 USD

### 1.2 å¯é€‰è®¾å¤‡

| è®¾å¤‡ | ç”¨é€” | ä»·æ ¼ |
|------|------|------|
| NVMe SSD é€‚é…å™¨ | æ›´å¿«çš„å­˜å‚¨é€Ÿåº¦ | $15 |
| NVMe SSD (256GB) | æ›¿ä»£ MicroSD | $30 |
| ä»¥å¤ªç½‘çº¿ | ç¨³å®šç½‘ç»œè¿æ¥ | $5 |

**æ¨èé…ç½®æ€»æˆæœ¬**: ~$175 USD

### 1.3 ä¸ºä»€ä¹ˆé€‰æ‹©æ ‘è“æ´¾ 5?

âœ… **è¶³å¤Ÿçš„å†…å­˜**: 8GB RAM å¯è¿è¡Œ Q4/Q8 é‡åŒ–çš„ 0.6B æ¨¡å‹  
âœ… **æ€§èƒ½æå‡**: CPU æ€§èƒ½æ¯”æ ‘è“æ´¾ 4 æå‡ 2-3 å€  
âœ… **ARM ä¼˜åŒ–**: æ–°ç‰ˆ llama.cpp å¯¹ ARM64 ä¼˜åŒ–è‰¯å¥½  
âœ… **ä½åŠŸè€—**: æŒç»­è¿è¡ŒåŠŸè€—ä»… 5-8W  
âœ… **ç¤¾åŒºæ”¯æŒ**: å¤§é‡æ•™ç¨‹å’Œä¼˜åŒ–æ–¹æ¡ˆ  

### 1.4 ä¸æ¨èçš„è®¾å¤‡

âŒ **æ ‘è“æ´¾ 4 (4GB)**: å†…å­˜ä¸è¶³ï¼Œå®¹æ˜“ OOM  
âŒ **æ ‘è“æ´¾ Zero**: æ€§èƒ½å¤ªå¼±  
âŒ **æ ‘è“æ´¾ 3**: ä¸æ”¯æŒ 64 ä½ç³»ç»Ÿï¼ˆéƒ¨åˆ†æ¨¡å‹éœ€è¦ï¼‰  

---

## 2. è½¯ä»¶ç¯å¢ƒå‡†å¤‡

### 2.1 æ“ä½œç³»ç»Ÿå®‰è£…

#### Step 1: ä¸‹è½½ Raspberry Pi OS

æ¨èä½¿ç”¨ **Raspberry Pi OS (64-bit) Lite** æˆ– **Desktop** ç‰ˆæœ¬

```bash
# ä¸‹è½½åœ°å€
https://www.raspberrypi.com/software/operating-systems/

# æ¨èç‰ˆæœ¬
Raspberry Pi OS Lite (64-bit) - æœ€æ–°ç‰ˆ
```

#### Step 2: çƒ§å½•ç³»ç»Ÿ

ä½¿ç”¨ **Raspberry Pi Imager** å·¥å…·ï¼š

1. ä¸‹è½½ Imager: https://www.raspberrypi.com/software/
2. é€‰æ‹©ç³»ç»Ÿé•œåƒ
3. é€‰æ‹© SD å¡
4. **é‡è¦**: ç‚¹å‡»è®¾ç½®å›¾æ ‡ âš™ï¸
   - å¯ç”¨ SSH
   - è®¾ç½®ç”¨æˆ·åå¯†ç 
   - é…ç½® WiFiï¼ˆå¯é€‰ï¼‰
5. ç‚¹å‡» "Write" çƒ§å½•

#### Step 3: é¦–æ¬¡å¯åŠ¨

```bash
# SSH è¿æ¥ï¼ˆæ›¿æ¢ä¸ºä½ çš„æ ‘è“æ´¾ IPï¼‰
ssh pi@raspberrypi.local
# æˆ–
ssh pi@192.168.1.xxx

# é¦–æ¬¡ç™»å½•åæ›´æ–°ç³»ç»Ÿ
sudo apt update && sudo apt upgrade -y

# å®‰è£…åŸºç¡€å·¥å…·
sudo apt install -y \
    build-essential \
    git \
    cmake \
    wget \
    curl \
    htop \
    vim \
    python3-pip
```

### 2.2 ç³»ç»Ÿä¼˜åŒ–

```bash
# 1. å¢åŠ  swap ç©ºé—´ï¼ˆé‡è¦ï¼ï¼‰
sudo dphys-swapfile swapoff
sudo nano /etc/dphys-swapfile
# ä¿®æ”¹ CONF_SWAPSIZE=2048 (2GB)
sudo dphys-swapfile setup
sudo dphys-swapfile swapon

# 2. å¯ç”¨æ€§èƒ½æ¨¡å¼ï¼ˆå¯é€‰ï¼‰
sudo raspi-config
# é€‰æ‹© Performance Options -> CPU Governor -> Performance

# 3. ç¦ç”¨ä¸å¿…è¦çš„æœåŠ¡
sudo systemctl disable bluetooth
sudo systemctl disable avahi-daemon

# 4. æ£€æŸ¥ç³»ç»Ÿä¿¡æ¯
uname -a  # ç¡®è®¤æ˜¯ aarch64
free -h   # ç¡®è®¤å†…å­˜å’Œ swap
```

---

## 3. æ¨¡å‹è½¬æ¢ä¸ä¼ è¾“

### 3.1 æ¨¡å‹æ ¼å¼è¯´æ˜

| æ ¼å¼ | ç”¨é€” | å¤§å° (0.6B) | æ¨ç†é€Ÿåº¦ |
|------|------|-------------|---------|
| **PyTorch (.safetensors)** | åŸå§‹è®­ç»ƒæ ¼å¼ | ~1.2GB | æ…¢ âŒ |
| **GGUF Q4_K_M** | llama.cpp æ¨è | ~400MB | å¿« âœ… |
| **GGUF Q8_0** | é«˜ç²¾åº¦ | ~650MB | ä¸­ç­‰ âš¡ |
| **GGUF Q2_K** | æå°å°ºå¯¸ | ~250MB | å¿«ä½†è´¨é‡é™ä½ âš ï¸ |

**æ¨è**: Q4_K_Mï¼ˆè´¨é‡å’Œé€Ÿåº¦çš„æœ€ä½³å¹³è¡¡ï¼‰

### 3.2 æ–¹æ³• 1: è®­ç»ƒæ—¶ç›´æ¥è½¬æ¢ï¼ˆæ¨èï¼‰

åœ¨å¾®è°ƒè„šæœ¬ä¸­å·²åŒ…å«ï¼š

```python
# è®­ç»ƒè„šæœ¬ä¼šè‡ªåŠ¨ç”Ÿæˆ GGUF
model.save_pretrained_gguf(
    OUTPUT_DIR + "/gguf",
    tokenizer,
    quantization_method="q4_k_m"
)
```

### 3.3 æ–¹æ³• 2: ä½¿ç”¨ llama.cpp è½¬æ¢

```bash
# 1. å…‹éš† llama.cpp
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp

# 2. ç¼–è¯‘ï¼ˆåœ¨ x86 æœºå™¨ä¸Šï¼‰
make

# 3. è½¬æ¢ PyTorch æ¨¡å‹ä¸º GGUF
python3 convert.py /path/to/your/model --outfile qwen3-0.6b-f16.gguf

# 4. é‡åŒ–ä¸º Q4_K_M
./quantize qwen3-0.6b-f16.gguf qwen3-0.6b-q4_k_m.gguf Q4_K_M
```

### 3.4 ä¼ è¾“æ¨¡å‹åˆ°æ ‘è“æ´¾

#### æ–¹æ³• A: SCP ä¼ è¾“

```bash
# åœ¨æœ¬åœ°æœºå™¨ä¸Šæ‰§è¡Œ
scp qwen3-0.6b-q4_k_m.gguf pi@raspberrypi.local:/home/pi/models/

# æˆ–ä½¿ç”¨å‹ç¼©ä¼ è¾“ï¼ˆæ›´å¿«ï¼‰
tar -czf model.tar.gz qwen3-0.6b-q4_k_m.gguf
scp model.tar.gz pi@raspberrypi.local:/home/pi/
ssh pi@raspberrypi.local "cd /home/pi && tar -xzf model.tar.gz"
```

#### æ–¹æ³• B: USB ä¼ è¾“

1. å°†æ¨¡å‹æ–‡ä»¶å¤åˆ¶åˆ° Uç›˜
2. Uç›˜æ’å…¥æ ‘è“æ´¾
3. æŒ‚è½½å¹¶å¤åˆ¶

```bash
# æ ‘è“æ´¾ä¸Šæ‰§è¡Œ
sudo mount /dev/sda1 /mnt
cp /mnt/qwen3-0.6b-q4_k_m.gguf /home/pi/models/
sudo umount /mnt
```

#### æ–¹æ³• C: Hugging Face Hub

```bash
# ä¸Šä¼ åˆ° Hugging Faceï¼ˆç§æœ‰ä»“åº“ï¼‰
huggingface-cli upload your-username/qwen3-0.6b-finetuned ./gguf

# åœ¨æ ‘è“æ´¾ä¸Šä¸‹è½½
huggingface-cli download your-username/qwen3-0.6b-finetuned \
    --local-dir /home/pi/models/
```

---

## 4. éƒ¨ç½²æ–¹æ¡ˆå¯¹æ¯”

| æ–¹æ¡ˆ | ä¼˜ç‚¹ | ç¼ºç‚¹ | æ¨èåº¦ |
|------|------|------|--------|
| **llama.cpp** | â€¢ åŸç”Ÿ C++ æ€§èƒ½<br>â€¢ å®Œå…¨æ§åˆ¶<br>â€¢ æ— é¢å¤–ä¾èµ– | â€¢ éœ€è¦ç¼–è¯‘<br>â€¢ API éœ€è¦è‡ªå·±å®ç° | â­â­â­â­â­ |
| **Ollama** | â€¢ ä¸€é”®å®‰è£…<br>â€¢ OpenAI å…¼å®¹ API<br>â€¢ æ˜“äºä½¿ç”¨ | â€¢ ç¨å¾®é‡é‡çº§<br>â€¢ æ›´æ–°è¾ƒæ…¢ | â­â­â­â­ |
| **llama-cpp-python** | â€¢ Python æ¥å£<br>â€¢ æ˜“äºé›†æˆ | â€¢ ä¾èµ–è¾ƒå¤š<br>â€¢ å†…å­˜å ç”¨é«˜ | â­â­â­ |

**æ¨è**: ç”Ÿäº§ç¯å¢ƒç”¨ **llama.cpp**ï¼Œå¿«é€Ÿæµ‹è¯•ç”¨ **Ollama**

---

## 5. æ–¹æ¡ˆA: llama.cpp éƒ¨ç½²

### 5.1 ç¼–è¯‘ llama.cpp

```bash
# SSH è¿æ¥åˆ°æ ‘è“æ´¾
ssh pi@raspberrypi.local

# åˆ›å»ºå·¥ä½œç›®å½•
mkdir -p ~/ai
cd ~/ai

# å…‹éš†ä»“åº“
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp

# ç¼–è¯‘ï¼ˆARM64 ä¼˜åŒ–ï¼‰
make -j4

# éªŒè¯ç¼–è¯‘æˆåŠŸ
./main --version
```

### 5.2 æµ‹è¯•æ¨ç†

```bash
# åˆ›å»ºæ¨¡å‹ç›®å½•
mkdir -p ~/models

# å‡è®¾æ¨¡å‹å·²ä¼ è¾“åˆ° ~/models/qwen3-0.6b-q4_k_m.gguf

# äº¤äº’å¼æµ‹è¯•
./main -m ~/models/qwen3-0.6b-q4_k_m.gguf \
    -n 128 \
    -p "What is the capital of France?" \
    --color

# æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯ï¼ˆverboseï¼‰
./main -m ~/models/qwen3-0.6b-q4_k_m.gguf \
    -n 128 \
    -p "Explain quantum computing in simple terms." \
    --verbose
```

### 5.3 æ€§èƒ½ä¼˜åŒ–å‚æ•°

```bash
# æ¨èé…ç½®ï¼ˆæ ‘è“æ´¾ 5ï¼‰
./main -m ~/models/qwen3-0.6b-q4_k_m.gguf \
    -t 4 \              # ä½¿ç”¨ 4 ä¸ªçº¿ç¨‹ï¼ˆæ ‘è“æ´¾ 5 æœ‰ 4 ä¸ªæ ¸å¿ƒï¼‰
    -c 2048 \           # ä¸Šä¸‹æ–‡é•¿åº¦
    -n 256 \            # ç”Ÿæˆæœ€å¤š 256 tokens
    --temp 0.7 \        # æ¸©åº¦å‚æ•°
    --top-p 0.9 \       # Top-p é‡‡æ ·
    --repeat-penalty 1.1 \  # é‡å¤æƒ©ç½š
    -p "ä½ çš„æç¤ºè¯"
```

### 5.4 å¯åŠ¨ HTTP æœåŠ¡å™¨

```bash
# å¯åŠ¨æœåŠ¡å™¨ï¼ˆç›‘å¬ 8080 ç«¯å£ï¼‰
./server -m ~/models/qwen3-0.6b-q4_k_m.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    -t 4 \
    -c 2048

# æµ‹è¯• API
curl http://localhost:8080/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "messages": [
            {"role": "user", "content": "Hello!"}
        ],
        "temperature": 0.7,
        "max_tokens": 100
    }'
```

---

## 6. æ–¹æ¡ˆB: Ollama éƒ¨ç½²

### 6.1 å®‰è£… Ollama

```bash
# ä¸€é”®å®‰è£…ï¼ˆå®˜æ–¹è„šæœ¬ï¼‰
curl -fsSL https://ollama.com/install.sh | sh

# æˆ–æ‰‹åŠ¨å®‰è£…
wget https://github.com/ollama/ollama/releases/download/v0.1.26/ollama-linux-arm64
sudo mv ollama-linux-arm64 /usr/local/bin/ollama
sudo chmod +x /usr/local/bin/ollama
```

### 6.2 åˆ›å»º Modelfile

```bash
# åˆ›å»º Modelfile
cat > ~/Modelfile << 'EOF'
FROM /home/pi/models/qwen3-0.6b-q4_k_m.gguf

TEMPLATE """{{ .System }}
{{ .Prompt }}"""

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER stop "<|im_end|>"
PARAMETER stop "<|endoftext|>"

SYSTEM """You are Qwen3, a helpful AI assistant fine-tuned on educational content."""
EOF
```

### 6.3 å¯¼å…¥æ¨¡å‹

```bash
# åˆ›å»º Ollama æ¨¡å‹
ollama create qwen3-finetuned -f ~/Modelfile

# åˆ—å‡ºæ¨¡å‹
ollama list

# æµ‹è¯•æ¨¡å‹
ollama run qwen3-finetuned "What is machine learning?"
```

### 6.4 å¯åŠ¨æœåŠ¡

```bash
# Ollama è‡ªåŠ¨åœ¨åå°è¿è¡Œï¼Œé»˜è®¤ç«¯å£ 11434

# æµ‹è¯• API
curl http://localhost:11434/api/generate -d '{
  "model": "qwen3-finetuned",
  "prompt": "Explain neural networks.",
  "stream": false
}'
```

---

## 7. æ€§èƒ½ä¼˜åŒ–

### 7.1 é¢„æœŸæ€§èƒ½

**æ ‘è“æ´¾ 5 (8GB) + Qwen3-0.6B Q4_K_M**

| æŒ‡æ ‡ | æ€§èƒ½ |
|------|------|
| æç¤ºè¯å¤„ç†é€Ÿåº¦ | ~60 tokens/s |
| ç”Ÿæˆé€Ÿåº¦ | 12-20 tokens/s |
| é¦– token å»¶è¿Ÿ | 500-1000ms |
| å†…å­˜å ç”¨ | ~800MB |
| CPU å ç”¨ | 60-80% |
| æ¸©åº¦ | 55-65Â°Cï¼ˆå¸¦é£æ‰‡ï¼‰ |

### 7.2 è°ƒä¼˜å»ºè®®

#### A. æ•£çƒ­ä¼˜åŒ–

```bash
# ç›‘æ§æ¸©åº¦
watch -n 1 'vcgencmd measure_temp'

# å¦‚æœæ¸©åº¦ > 70Â°Cï¼Œè€ƒè™‘ï¼š
# 1. æ·»åŠ æ•£çƒ­ç‰‡
# 2. å®‰è£…ä¸»åŠ¨é£æ‰‡
# 3. æ”¹å–„æœºç®±é€šé£
```

#### B. å†…å­˜ç®¡ç†

```bash
# ç›‘æ§å†…å­˜
htop

# å¦‚æœå†…å­˜ä¸è¶³ï¼š
# 1. ä½¿ç”¨æ›´å°çš„é‡åŒ–ï¼ˆQ4 -> Q2ï¼‰
# 2. å‡å°ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆ-c 1024ï¼‰
# 3. å…³é—­ä¸å¿…è¦çš„æœåŠ¡
```

#### C. CPU ä¼˜åŒ–

```bash
# è®¾ç½®æ€§èƒ½æ¨¡å¼
echo performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor

# æŸ¥çœ‹é¢‘ç‡
cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_cur_freq
```

### 7.3 æ‰¹é‡æ¨ç†ä¼˜åŒ–

å¯¹äºæ‰¹é‡ä»»åŠ¡ï¼Œä½¿ç”¨æ‰¹å¤„ç†æ¨¡å¼ï¼š

```bash
# åˆ›å»ºè¾“å…¥æ–‡ä»¶
cat > prompts.txt << EOF
What is AI?
Explain deep learning.
What is NLP?
EOF

# æ‰¹å¤„ç†
while read -r prompt; do
    ./main -m ~/models/qwen3-0.6b-q4_k_m.gguf \
        -n 100 -p "$prompt" >> results.txt
done < prompts.txt
```

---

## 8. å¸¸è§é—®é¢˜

### Q1: æ ‘è“æ´¾è¿è¡Œæ¨¡å‹å¾ˆæ…¢æ€ä¹ˆåŠï¼Ÿ

**A**: 
1. ç¡®è®¤ä½¿ç”¨ Q4_K_M æˆ–æ›´å°é‡åŒ–
2. å‡å°ä¸Šä¸‹æ–‡é•¿åº¦ (`-c 1024`)
3. æ£€æŸ¥æ˜¯å¦å¯ç”¨æ€§èƒ½æ¨¡å¼
4. ç¡®ä¿æ•£çƒ­è‰¯å¥½

### Q2: å†…å­˜ä¸è¶³ï¼ˆOOMï¼‰æ€ä¹ˆåŠï¼Ÿ

**A**:
1. å¢åŠ  swap ç©ºé—´åˆ° 4GB
2. ä½¿ç”¨ Q2_K é‡åŒ–
3. å…³é—­æ¡Œé¢ç¯å¢ƒï¼ˆæ”¹ç”¨ SSHï¼‰
4. å‡å°çº¿ç¨‹æ•° (`-t 2`)

### Q3: æ¨¡å‹è¾“å‡ºè´¨é‡ä¸ä½³ï¼Ÿ

**A**:
1. è°ƒæ•´æ¸©åº¦å‚æ•° (`--temp 0.5-1.0`)
2. ä½¿ç”¨ Q8_0 é‡åŒ–ï¼ˆè´¨é‡æ›´å¥½ï¼‰
3. å¢åŠ ä¸Šä¸‹æ–‡é•¿åº¦
4. æ£€æŸ¥å¾®è°ƒæ•°æ®è´¨é‡

### Q4: å¦‚ä½•å®ç°è‡ªåŠ¨å¯åŠ¨ï¼Ÿ

**A**:
```bash
# åˆ›å»º systemd æœåŠ¡
sudo nano /etc/systemd/system/llama-server.service

# å†…å®¹:
[Unit]
Description=Llama.cpp Server
After=network.target

[Service]
Type=simple
User=pi
WorkingDirectory=/home/pi/llama.cpp
ExecStart=/home/pi/llama.cpp/server -m /home/pi/models/qwen3-0.6b-q4_k_m.gguf --host 0.0.0.0 --port 8080 -t 4
Restart=always

[Install]
WantedBy=multi-user.target

# å¯ç”¨æœåŠ¡
sudo systemctl enable llama-server
sudo systemctl start llama-server
sudo systemctl status llama-server
```

### Q5: å¦‚ä½•è¿œç¨‹è®¿é—®ï¼Ÿ

**A**:
```bash
# æ–¹æ¡ˆ 1: SSH éš§é“
ssh -L 8080:localhost:8080 pi@raspberrypi.local

# æ–¹æ¡ˆ 2: é…ç½®é˜²ç«å¢™
sudo ufw allow 8080

# æ–¹æ¡ˆ 3: Tailscale VPNï¼ˆæ¨èï¼‰
curl -fsSL https://tailscale.com/install.sh | sh
sudo tailscale up
```

---

## 9. å®Œæ•´éƒ¨ç½²è„šæœ¬

è¯¦è§åç»­ artifacts ä¸­çš„è‡ªåŠ¨åŒ–éƒ¨ç½²è„šæœ¬ã€‚

---

## 10. å‚è€ƒèµ„æº

- **llama.cpp**: https://github.com/ggerganov/llama.cpp
- **Ollama**: https://ollama.com/
- **æ ‘è“æ´¾å®˜æ–¹**: https://www.raspberrypi.com/
- **Qwen3 æ¨¡å‹**: https://huggingface.co/Qwen/Qwen3-0.6B
- **ä¼˜åŒ–æŒ‡å—**: https://www.raspberrypi.com/documentation/computers/config_txt.html

---

**ç¥éƒ¨ç½²æˆåŠŸï¼ğŸ‰**