#!/bin/bash
################################################################################
# Qwen3-0.6B æ ‘èŽ“æ´¾è‡ªåŠ¨éƒ¨ç½²è„šæœ¬
# é€‚ç”¨äºŽ: Raspberry Pi 5 (8GB) + Raspberry Pi OS 64-bit
# ç”¨é€”: è‡ªåŠ¨å®‰è£… llama.cpp å¹¶éƒ¨ç½²å¾®è°ƒåŽçš„æ¨¡åž‹
################################################################################

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# é…ç½®å‚æ•°
LLAMA_CPP_DIR="$HOME/llama.cpp"
MODEL_DIR="$HOME/models"
MODEL_NAME="qwen3-0.6b-q4_k_m.gguf"
MODEL_PATH="$MODEL_DIR/$MODEL_NAME"
SERVER_PORT=8080
NUM_THREADS=4
CONTEXT_SIZE=2048

################################################################################
# è¾…åŠ©å‡½æ•°
################################################################################

print_header() {
    echo -e "${BLUE}"
    echo "================================================================================"
    echo "$1"
    echo "================================================================================"
    echo -e "${NC}"
}

print_success() {
    echo -e "${GREEN}âœ“ $1${NC}"
}

print_error() {
    echo -e "${RED}âœ— $1${NC}"
}

print_info() {
    echo -e "${YELLOW}â„¹ $1${NC}"
}

check_command() {
    if command -v "$1" &> /dev/null; then
        print_success "$1 å·²å®‰è£…"
        return 0
    else
        print_error "$1 æœªå®‰è£…"
        return 1
    fi
}

################################################################################
# ä¸»è¦åŠŸèƒ½å‡½æ•°
################################################################################

check_system() {
    print_header "æ­¥éª¤ 1: æ£€æŸ¥ç³»ç»ŸçŽ¯å¢ƒ"
    
    # æ£€æŸ¥æž¶æž„
    ARCH=$(uname -m)
    if [ "$ARCH" != "aarch64" ]; then
        print_error "ä¸æ”¯æŒçš„æž¶æž„: $ARCH (éœ€è¦ aarch64)"
        exit 1
    fi
    print_success "æž¶æž„: $ARCH"
    
    # æ£€æŸ¥å†…å­˜
    TOTAL_MEM=$(free -g | awk '/^Mem:/{print $2}')
    if [ "$TOTAL_MEM" -lt 6 ]; then
        print_error "å†…å­˜ä¸è¶³: ${TOTAL_MEM}GB (å»ºè®®è‡³å°‘ 8GB)"
        print_info "éƒ¨ç½²å¯èƒ½å¤±è´¥ï¼Œæ˜¯å¦ç»§ç»­? (y/n)"
        read -r CONTINUE
        if [ "$CONTINUE" != "y" ]; then
            exit 1
        fi
    else
        print_success "å†…å­˜: ${TOTAL_MEM}GB"
    fi
    
    # æ£€æŸ¥æ“ä½œç³»ç»Ÿ
    if [ -f /etc/os-release ]; then
        . /etc/os-release
        print_success "æ“ä½œç³»ç»Ÿ: $PRETTY_NAME"
    fi
    
    # æ£€æŸ¥ swap
    SWAP=$(free -g | awk '/^Swap:/{print $2}')
    print_info "Swap: ${SWAP}GB"
    if [ "$SWAP" -lt 2 ]; then
        print_info "å»ºè®®å¢žåŠ  swap ç©ºé—´è‡³å°‘ 2GB"
    fi
    
    echo ""
}

install_dependencies() {
    print_header "æ­¥éª¤ 2: å®‰è£…ä¾èµ–"
    
    print_info "æ›´æ–°è½¯ä»¶åŒ…åˆ—è¡¨..."
    sudo apt update
    
    print_info "å®‰è£…ç¼–è¯‘å·¥å…·..."
    sudo apt install -y \
        build-essential \
        git \
        cmake \
        wget \
        curl \
        htop \
        vim \
        python3-pip
    
    print_success "ä¾èµ–å®‰è£…å®Œæˆ"
    echo ""
}

setup_swap() {
    print_header "æ­¥éª¤ 3: é…ç½® Swap ç©ºé—´"
    
    CURRENT_SWAP=$(free -g | awk '/^Swap:/{print $2}')
    if [ "$CURRENT_SWAP" -ge 2 ]; then
        print_success "Swap å·²é…ç½®: ${CURRENT_SWAP}GB"
        return
    fi
    
    print_info "å¢žåŠ  Swap åˆ° 2GB..."
    
    sudo dphys-swapfile swapoff || true
    
    # å¤‡ä»½åŽŸé…ç½®
    sudo cp /etc/dphys-swapfile /etc/dphys-swapfile.backup
    
    # è®¾ç½®ä¸º 2GB
    sudo sed -i 's/^CONF_SWAPSIZE=.*/CONF_SWAPSIZE=2048/' /etc/dphys-swapfile
    
    sudo dphys-swapfile setup
    sudo dphys-swapfile swapon
    
    NEW_SWAP=$(free -g | awk '/^Swap:/{print $2}')
    print_success "Swap é…ç½®å®Œæˆ: ${NEW_SWAP}GB"
    echo ""
}

compile_llama_cpp() {
    print_header "æ­¥éª¤ 4: ç¼–è¯‘ llama.cpp"
    
    if [ -d "$LLAMA_CPP_DIR" ]; then
        print_info "æ£€æµ‹åˆ°çŽ°æœ‰ llama.cpp ç›®å½•"
        print_info "æ˜¯å¦é‡æ–°ç¼–è¯‘? (y/n)"
        read -r RECOMPILE
        if [ "$RECOMPILE" = "y" ]; then
            rm -rf "$LLAMA_CPP_DIR"
        else
            print_success "è·³è¿‡ç¼–è¯‘"
            return
        fi
    fi
    
    print_info "å…‹éš† llama.cpp ä»“åº“..."
    git clone https://github.com/ggerganov/llama.cpp "$LLAMA_CPP_DIR"
    
    cd "$LLAMA_CPP_DIR"
    
    print_info "å¼€å§‹ç¼–è¯‘ (å¯èƒ½éœ€è¦ 5-10 åˆ†é’Ÿ)..."
    make -j4
    
    if [ -f "./main" ]; then
        print_success "llama.cpp ç¼–è¯‘æˆåŠŸ"
        ./main --version
    else
        print_error "ç¼–è¯‘å¤±è´¥"
        exit 1
    fi
    
    cd - > /dev/null
    echo ""
}

setup_model_dir() {
    print_header "æ­¥éª¤ 5: è®¾ç½®æ¨¡åž‹ç›®å½•"
    
    if [ ! -d "$MODEL_DIR" ]; then
        mkdir -p "$MODEL_DIR"
        print_success "åˆ›å»ºæ¨¡åž‹ç›®å½•: $MODEL_DIR"
    else
        print_success "æ¨¡åž‹ç›®å½•å·²å­˜åœ¨: $MODEL_DIR"
    fi
    
    echo ""
}

check_model() {
    print_header "æ­¥éª¤ 6: æ£€æŸ¥æ¨¡åž‹æ–‡ä»¶"
    
    if [ -f "$MODEL_PATH" ]; then
        print_success "æ‰¾åˆ°æ¨¡åž‹æ–‡ä»¶: $MODEL_PATH"
        MODEL_SIZE=$(du -h "$MODEL_PATH" | cut -f1)
        print_info "æ¨¡åž‹å¤§å°: $MODEL_SIZE"
    else
        print_error "æ¨¡åž‹æ–‡ä»¶ä¸å­˜åœ¨: $MODEL_PATH"
        print_info ""
        print_info "è¯·å°†æ¨¡åž‹æ–‡ä»¶ä¸Šä¼ åˆ°: $MODEL_DIR"
        print_info "æ–‡ä»¶ååº”ä¸º: $MODEL_NAME"
        print_info ""
        print_info "ä¸Šä¼ æ–¹æ³•:"
        print_info "1. SCP: scp your-model.gguf pi@raspberrypi.local:$MODEL_DIR/$MODEL_NAME"
        print_info "2. USB: æŒ‚è½½ Uç›˜åŽå¤åˆ¶"
        print_info "3. ç½‘ç»œä¸‹è½½: wget/curl ä¸‹è½½åˆ°è¯¥ç›®å½•"
        print_info ""
        exit 1
    fi
    
    echo ""
}

test_inference() {
    print_header "æ­¥éª¤ 7: æµ‹è¯•æŽ¨ç†"
    
    print_info "è¿è¡Œç®€å•æŽ¨ç†æµ‹è¯•..."
    
    cd "$LLAMA_CPP_DIR"
    
    PROMPT="What is artificial intelligence?"
    
    print_info "æç¤ºè¯: $PROMPT"
    print_info "ç”Ÿæˆä¸­..."
    echo ""
    
    ./main -m "$MODEL_PATH" \
        -n 100 \
        -t "$NUM_THREADS" \
        -c "$CONTEXT_SIZE" \
        -p "$PROMPT" \
        --color
    
    echo ""
    print_success "æŽ¨ç†æµ‹è¯•å®Œæˆ"
    
    cd - > /dev/null
    echo ""
}

create_systemd_service() {
    print_header "æ­¥éª¤ 8: åˆ›å»ºç³»ç»ŸæœåŠ¡"
    
    print_info "æ˜¯å¦åˆ›å»ºè‡ªåŠ¨å¯åŠ¨æœåŠ¡? (y/n)"
    read -r CREATE_SERVICE
    
    if [ "$CREATE_SERVICE" != "y" ]; then
        print_info "è·³è¿‡æœåŠ¡åˆ›å»º"
        return
    fi
    
    SERVICE_FILE="/etc/systemd/system/llama-server.service"
    
    print_info "åˆ›å»º systemd æœåŠ¡æ–‡ä»¶..."
    
    sudo tee "$SERVICE_FILE" > /dev/null <<EOF
[Unit]
Description=Llama.cpp HTTP Server
After=network.target

[Service]
Type=simple
User=$USER
WorkingDirectory=$LLAMA_CPP_DIR
ExecStart=$LLAMA_CPP_DIR/server \\
    -m $MODEL_PATH \\
    --host 0.0.0.0 \\
    --port $SERVER_PORT \\
    -t $NUM_THREADS \\
    -c $CONTEXT_SIZE
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
EOF
    
    print_success "æœåŠ¡æ–‡ä»¶å·²åˆ›å»º: $SERVICE_FILE"
    
    # é‡è½½ systemd
    sudo systemctl daemon-reload
    
    # å¯ç”¨æœåŠ¡
    sudo systemctl enable llama-server
    print_success "æœåŠ¡å·²è®¾ç½®ä¸ºå¼€æœºè‡ªå¯"
    
    # å¯åŠ¨æœåŠ¡
    sudo systemctl start llama-server
    print_success "æœåŠ¡å·²å¯åŠ¨"
    
    sleep 2
    
    # æ£€æŸ¥çŠ¶æ€
    if sudo systemctl is-active --quiet llama-server; then
        print_success "æœåŠ¡è¿è¡Œæ­£å¸¸"
    else
        print_error "æœåŠ¡å¯åŠ¨å¤±è´¥"
        print_info "æŸ¥çœ‹æ—¥å¿—: sudo journalctl -u llama-server -f"
    fi
    
    echo ""
}

create_helper_scripts() {
    print_header "æ­¥éª¤ 9: åˆ›å»ºè¾…åŠ©è„šæœ¬"
    
    # åˆ›å»ºå¿«é€Ÿå¯åŠ¨è„šæœ¬
    cat > "$HOME/start_llama.sh" <<'EOF'
#!/bin/bash
cd ~/llama.cpp
./server -m ~/models/qwen3-0.6b-q4_k_m.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    -t 4 \
    -c 2048
EOF
    chmod +x "$HOME/start_llama.sh"
    print_success "åˆ›å»ºå¯åŠ¨è„šæœ¬: ~/start_llama.sh"
    
    # åˆ›å»ºäº¤äº’æµ‹è¯•è„šæœ¬
    cat > "$HOME/test_llama.sh" <<'EOF'
#!/bin/bash
cd ~/llama.cpp
./main -m ~/models/qwen3-0.6b-q4_k_m.gguf \
    -n 256 \
    -t 4 \
    -c 2048 \
    -p "$1" \
    --color
EOF
    chmod +x "$HOME/test_llama.sh"
    print_success "åˆ›å»ºæµ‹è¯•è„šæœ¬: ~/test_llama.sh"
    
    # åˆ›å»º API æµ‹è¯•è„šæœ¬
    cat > "$HOME/test_api.sh" <<'EOF'
#!/bin/bash
curl http://localhost:8080/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d "{
        \"messages\": [
            {\"role\": \"user\", \"content\": \"$1\"}
        ],
        \"temperature\": 0.7,
        \"max_tokens\": 100
    }" | jq
EOF
    chmod +x "$HOME/test_api.sh"
    print_success "åˆ›å»º API æµ‹è¯•è„šæœ¬: ~/test_api.sh"
    
    echo ""
}

print_summary() {
    print_header "éƒ¨ç½²å®Œæˆ! ðŸŽ‰"
    
    echo -e "${GREEN}éƒ¨ç½²ä¿¡æ¯:${NC}"
    echo "  â€¢ llama.cpp ç›®å½•: $LLAMA_CPP_DIR"
    echo "  â€¢ æ¨¡åž‹ç›®å½•: $MODEL_DIR"
    echo "  â€¢ æ¨¡åž‹æ–‡ä»¶: $MODEL_PATH"
    echo "  â€¢ æœåŠ¡ç«¯å£: $SERVER_PORT"
    echo ""
    
    echo -e "${YELLOW}ä½¿ç”¨æ–¹æ³•:${NC}"
    echo "  1. äº¤äº’å¼æµ‹è¯•:"
    echo "     cd $LLAMA_CPP_DIR"
    echo "     ./main -m $MODEL_PATH -n 128 -p \"ä½ çš„é—®é¢˜\""
    echo ""
    echo "  2. å¯åŠ¨ HTTP æœåŠ¡å™¨:"
    echo "     ~/start_llama.sh"
    echo "     æˆ–"
    echo "     sudo systemctl start llama-server"
    echo ""
    echo "  3. æµ‹è¯• API (éœ€è¦å…ˆå¯åŠ¨æœåŠ¡å™¨):"
    echo "     ~/test_api.sh \"Hello, AI!\""
    echo ""
    echo "  4. å¿«é€Ÿæµ‹è¯•:"
    echo "     ~/test_llama.sh \"What is machine learning?\""
    echo ""
    
    echo -e "${YELLOW}æ€§èƒ½ç›‘æŽ§:${NC}"
    echo "  â€¢ æŸ¥çœ‹ CPU/å†…å­˜: htop"
    echo "  â€¢ æŸ¥çœ‹æ¸©åº¦: vcgencmd measure_temp"
    echo "  â€¢ æŸ¥çœ‹æœåŠ¡æ—¥å¿—: sudo journalctl -u llama-server -f"
    echo ""
    
    echo -e "${YELLOW}è¿œç¨‹è®¿é—®:${NC}"
    echo "  â€¢ æœ¬åœ°ç½‘ç»œ: http://raspberrypi.local:$SERVER_PORT"
    echo "  â€¢ IP åœ°å€: http://$(hostname -I | awk '{print $1}'):$SERVER_PORT"
    echo ""
    
    echo -e "${GREEN}éƒ¨ç½²æˆåŠŸå®Œæˆ!${NC}"
}

################################################################################
# ä¸»æµç¨‹
################################################################################

main() {
    clear
    print_header "Qwen3-0.6B æ ‘èŽ“æ´¾è‡ªåŠ¨éƒ¨ç½²è„šæœ¬"
    
    print_info "å¼€å§‹éƒ¨ç½²æµç¨‹..."
    sleep 2
    
    check_system
    install_dependencies
    setup_swap
    compile_llama_cpp
    setup_model_dir
    check_model
    test_inference
    create_systemd_service
    create_helper_scripts
    print_summary
}

# è¿è¡Œä¸»å‡½æ•°
main