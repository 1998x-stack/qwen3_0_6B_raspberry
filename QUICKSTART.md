# ğŸš€ Qwen3-0.6B æ ‘è“æ´¾éƒ¨ç½² - å¿«é€Ÿå…¥é—¨æŒ‡å—

## ğŸ“‹ å‰ç½®æ£€æŸ¥æ¸…å•

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿ï¼š

- âœ… æ ‘è“æ´¾ 5 (8GB RAM)
- âœ… Raspberry Pi OS 64-bit å·²å®‰è£…
- âœ… è‡³å°‘ 10GB å¯ç”¨ç©ºé—´
- âœ… ç¨³å®šçš„ç½‘ç»œè¿æ¥
- âœ… SSH å·²å¯ç”¨ï¼ˆæ¨èï¼‰

---

## ğŸ¯ 5 åˆ†é’Ÿå¿«é€Ÿéƒ¨ç½²

### Step 1: è®­ç»ƒæ¨¡å‹ï¼ˆåœ¨ GPU æœºå™¨ä¸Šï¼‰

```bash
# 1. å…‹éš†é¡¹ç›®
git clone <your-repo>
cd qwen3-finetune

# 2. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 3. è¿è¡Œå¾®è°ƒï¼ˆå¤§çº¦ 4-12 å°æ—¶ï¼Œå–å†³äº GPUï¼‰
python train_qwen3_fineweb.py

# 4. æ¨¡å‹ä¼šè‡ªåŠ¨ä¿å­˜åˆ° ./outputs/qwen3-0.6b-fineweb-edu/gguf/
```

### Step 2: ä¼ è¾“æ¨¡å‹åˆ°æ ‘è“æ´¾

```bash
# åœ¨æœ¬åœ°æœºå™¨ä¸Šæ‰§è¡Œ
scp outputs/qwen3-0.6b-fineweb-edu/gguf/*.gguf \
    pi@raspberrypi.local:/home/pi/models/qwen3-0.6b-q4_k_m.gguf
```

### Step 3: è‡ªåŠ¨éƒ¨ç½²ï¼ˆåœ¨æ ‘è“æ´¾ä¸Šï¼‰

```bash
# SSH è¿æ¥åˆ°æ ‘è“æ´¾
ssh pi@raspberrypi.local

# ä¸‹è½½éƒ¨ç½²è„šæœ¬
wget https://raw.githubusercontent.com/<your-repo>/deploy_raspberry_pi.sh

# æ·»åŠ æ‰§è¡Œæƒé™
chmod +x deploy_raspberry_pi.sh

# è¿è¡Œéƒ¨ç½²è„šæœ¬ï¼ˆè‡ªåŠ¨å®Œæˆæ‰€æœ‰é…ç½®ï¼‰
./deploy_raspberry_pi.sh
```

### Step 4: æµ‹è¯•æ¨ç†

```bash
# æ–¹æ³• 1: å‘½ä»¤è¡Œæµ‹è¯•
~/test_llama.sh "What is artificial intelligence?"

# æ–¹æ³• 2: å¯åŠ¨ HTTP æœåŠ¡å™¨
~/start_llama.sh

# æ–¹æ³• 3: ä½¿ç”¨ C++ å®¢æˆ·ç«¯
cd ~/
wget <simple_client.cpp>
make
./simple_client "Hello, AI!"
```

---

## ğŸ“Š é¢„æœŸæ€§èƒ½

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| é¦– token å»¶è¿Ÿ | ~500-1000ms |
| ç”Ÿæˆé€Ÿåº¦ | 12-20 tokens/s |
| å†…å­˜å ç”¨ | ~800MB |
| CPU ä½¿ç”¨ç‡ | 60-80% |

---

## ğŸ”§ å¸¸ç”¨å‘½ä»¤

### æœåŠ¡ç®¡ç†

```bash
# å¯åŠ¨æœåŠ¡
sudo systemctl start llama-server

# åœæ­¢æœåŠ¡
sudo systemctl stop llama-server

# æŸ¥çœ‹çŠ¶æ€
sudo systemctl status llama-server

# æŸ¥çœ‹æ—¥å¿—
sudo journalctl -u llama-server -f
```

### æ€§èƒ½ç›‘æ§

```bash
# CPU å’Œå†…å­˜
htop

# æ¸©åº¦
watch -n 1 vcgencmd measure_temp

# GPU å†…å­˜ï¼ˆæ ‘è“æ´¾ï¼‰
vcgencmd get_mem arm && vcgencmd get_mem gpu
```

### API æµ‹è¯•

```bash
# ç®€å•æµ‹è¯•
curl http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "Hi!"}],
    "max_tokens": 100
  }'

# ä½¿ç”¨ jq æ ¼å¼åŒ–è¾“å‡º
~/test_api.sh "Explain quantum computing"
```

---

## ğŸ› æ•…éšœæ’é™¤

### é—®é¢˜ 1: å†…å­˜ä¸è¶³

```bash
# å¢åŠ  swap
sudo dphys-swapfile swapoff
sudo nano /etc/dphys-swapfile
# è®¾ç½® CONF_SWAPSIZE=4096
sudo dphys-swapfile setup
sudo dphys-swapfile swapon
```

### é—®é¢˜ 2: æ¸©åº¦è¿‡é«˜

```bash
# æ£€æŸ¥æ¸©åº¦
vcgencmd measure_temp

# å¦‚æœ > 70Â°C:
# 1. æ·»åŠ é£æ‰‡
# 2. æ”¹å–„é€šé£
# 3. é™ä½çº¿ç¨‹æ•°: ç¼–è¾‘ ~/start_llama.shï¼Œæ”¹ -t 4 ä¸º -t 2
```

### é—®é¢˜ 3: æ¨ç†é€Ÿåº¦æ…¢

```bash
# ä¼˜åŒ–æ–¹æ¡ˆ:
# 1. ä½¿ç”¨æ›´å°çš„é‡åŒ–ï¼ˆQ4 -> Q2ï¼‰
# 2. å‡å°ä¸Šä¸‹æ–‡é•¿åº¦
# 3. å¯ç”¨æ€§èƒ½æ¨¡å¼
echo performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor
```

### é—®é¢˜ 4: æœåŠ¡æ— æ³•å¯åŠ¨

```bash
# æŸ¥çœ‹é”™è¯¯æ—¥å¿—
sudo journalctl -u llama-server -n 50 --no-pager

# æ‰‹åŠ¨æµ‹è¯•
cd ~/llama.cpp
./server -m ~/models/qwen3-0.6b-q4_k_m.gguf --port 8080 -t 4
```

---

## ğŸŒ è¿œç¨‹è®¿é—®è®¾ç½®

### æ–¹æ³• 1: SSH éš§é“ï¼ˆæœ€å®‰å…¨ï¼‰

```bash
# åœ¨æœ¬åœ°æœºå™¨ä¸Šæ‰§è¡Œ
ssh -L 8080:localhost:8080 pi@raspberrypi.local

# ç„¶åè®¿é—®: http://localhost:8080
```

### æ–¹æ³• 2: å±€åŸŸç½‘è®¿é—®

```bash
# åœ¨æ ‘è“æ´¾ä¸ŠæŸ¥çœ‹ IP
hostname -I

# åœ¨åŒä¸€å±€åŸŸç½‘çš„è®¾å¤‡ä¸Šè®¿é—®
# http://<æ ‘è“æ´¾IP>:8080
```

### æ–¹æ³• 3: Tailscale VPNï¼ˆæ¨èï¼‰

```bash
# åœ¨æ ‘è“æ´¾ä¸Šå®‰è£… Tailscale
curl -fsSL https://tailscale.com/install.sh | sh
sudo tailscale up

# åœ¨å…¶ä»–è®¾å¤‡ä¸Šä¹Ÿå®‰è£… Tailscale
# ç„¶åå¯ä»¥é€šè¿‡ Tailscale IP è®¿é—®
```

---

## ğŸ“š è¿›é˜¶é…ç½®

### è‡ªå®šä¹‰æç¤ºè¯æ¨¡æ¿

ç¼–è¾‘ `~/Modelfile` æ·»åŠ ç³»ç»Ÿæç¤ºè¯ï¼š

```
SYSTEM """You are a helpful AI assistant specialized in [your domain].
Please provide concise and accurate answers."""
```

### è°ƒæ•´ç”Ÿæˆå‚æ•°

```bash
# åœ¨å¯åŠ¨è„šæœ¬ä¸­æ·»åŠ å‚æ•°
./server -m model.gguf \
    --temp 0.8 \           # åˆ›æ„åº¦ï¼ˆ0.0-2.0ï¼‰
    --top-p 0.95 \         # é‡‡æ ·å¤šæ ·æ€§
    --repeat-penalty 1.1 \ # å‡å°‘é‡å¤
    --ctx-size 4096        # æ›´é•¿çš„ä¸Šä¸‹æ–‡
```

### æ‰¹é‡æ¨ç†

```bash
# åˆ›å»ºæ‰¹å¤„ç†è„šæœ¬
cat > batch_inference.sh << 'EOF'
#!/bin/bash
while IFS= read -r prompt; do
    echo "å¤„ç†: $prompt"
    ~/test_llama.sh "$prompt" >> results.txt
    echo "---" >> results.txt
done < prompts.txt
EOF

chmod +x batch_inference.sh
```

---

## ğŸ”— æœ‰ç”¨çš„é“¾æ¥

- **llama.cpp æ–‡æ¡£**: https://github.com/ggerganov/llama.cpp
- **Qwen3 æ¨¡å‹å¡**: https://huggingface.co/Qwen/Qwen3-0.6B
- **æ ‘è“æ´¾å®˜æ–¹è®ºå›**: https://forums.raspberrypi.com/
- **FineWeb-Edu**: https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu

---

## ğŸ“ ä¸‹ä¸€æ­¥

1. âœ… **ä¼˜åŒ–æ€§èƒ½**: è°ƒæ•´çº¿ç¨‹æ•°ã€ä¸Šä¸‹æ–‡å¤§å°
2. âœ… **é›†æˆåº”ç”¨**: æ„å»º Web UI æˆ–è¯­éŸ³åŠ©æ‰‹
3. âœ… **æŒç»­å­¦ä¹ **: æ”¶é›†åé¦ˆæ•°æ®è¿›è¡Œå¢é‡è®­ç»ƒ
4. âœ… **é›†ç¾¤éƒ¨ç½²**: å¤šä¸ªæ ‘è“æ´¾è´Ÿè½½å‡è¡¡

---

## ğŸ’¡ æç¤º

- é¦–æ¬¡è¿è¡Œå¯èƒ½éœ€è¦é¢„çƒ­ï¼Œåç»­æ¨ç†ä¼šæ›´å¿«
- å®šæœŸæ¸…ç†æ—¥å¿—é¿å…å æ»¡å­˜å‚¨ï¼š`sudo journalctl --vacuum-time=7d`
- ä½¿ç”¨ `screen` æˆ– `tmux` ä¿æŒé•¿æ—¶é—´è¿è¡Œ
- å¤‡ä»½é‡è¦çš„æ¨¡å‹æ–‡ä»¶åˆ°å¤–éƒ¨å­˜å‚¨

---

**ğŸ‰ äº«å—ä½ çš„æœ¬åœ° AI åŠ©æ‰‹ï¼**

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æŸ¥é˜…å®Œæ•´æ–‡æ¡£æˆ–æäº¤ Issueã€‚