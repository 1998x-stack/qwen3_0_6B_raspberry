#!/usr/bin/env python3
"""
Qwen3-0.6B åœ¨ FineWeb-Edu æ•°æ®é›†ä¸Šçš„å¾®è°ƒè„šæœ¬
ä½¿ç”¨ Unsloth æ¡†æ¶è¿›è¡Œé«˜æ•ˆè®­ç»ƒ
"""

import os
import torch
from datasets import load_dataset
from transformers import TrainingArguments
from trl import SFTTrainer
from unsloth import FastLanguageModel, is_bfloat16_supported

# ============================================================================
# é…ç½®å‚æ•°
# ============================================================================

# æ¨¡å‹é…ç½®
MODEL_NAME = "Qwen/Qwen3-0.6B"
MAX_SEQ_LENGTH = 2048
LOAD_IN_4BIT = True

# LoRA é…ç½®
LORA_R = 16
LORA_ALPHA = 32
LORA_DROPOUT = 0.05
TARGET_MODULES = ["q_proj", "k_proj", "v_proj", "o_proj", 
                  "gate_proj", "up_proj", "down_proj"]

# æ•°æ®é›†é…ç½®
DATASET_NAME = "HuggingFaceFW/fineweb-edu"
DATASET_SPLIT = "train"
NUM_SAMPLES = 100000  # é‡‡æ ·æ•°é‡ï¼Œæ ¹æ®èµ„æºè°ƒæ•´
TEXT_COLUMN = "text"

# è®­ç»ƒé…ç½®
OUTPUT_DIR = "./outputs/qwen3-0.6b-fineweb-edu"
NUM_TRAIN_EPOCHS = 1
PER_DEVICE_TRAIN_BATCH_SIZE = 4
GRADIENT_ACCUMULATION_STEPS = 4
LEARNING_RATE = 2e-4
WEIGHT_DECAY = 0.01
WARMUP_STEPS = 100
LOGGING_STEPS = 10
SAVE_STEPS = 500

# ä¼˜åŒ–é…ç½®
USE_GRADIENT_CHECKPOINTING = True
OPTIM = "adamw_8bit"
FP16 = not is_bfloat16_supported()
BF16 = is_bfloat16_supported()

# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    print("=" * 80)
    print("Qwen3-0.6B FineWeb-Edu å¾®è°ƒè„šæœ¬")
    print("=" * 80)
    
    # 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    print("\n[1/5] åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=MODEL_NAME,
        max_seq_length=MAX_SEQ_LENGTH,
        dtype=None,  # è‡ªåŠ¨æ£€æµ‹
        load_in_4bit=LOAD_IN_4BIT,
    )
    print(f"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ: {MODEL_NAME}")
    print(f"  - æœ€å¤§åºåˆ—é•¿åº¦: {MAX_SEQ_LENGTH}")
    print(f"  - 4-bit é‡åŒ–: {LOAD_IN_4BIT}")
    
    # 2. æ·»åŠ  LoRA é€‚é…å™¨
    print("\n[2/5] æ·»åŠ  LoRA é€‚é…å™¨...")
    model = FastLanguageModel.get_peft_model(
        model,
        r=LORA_R,
        target_modules=TARGET_MODULES,
        lora_alpha=LORA_ALPHA,
        lora_dropout=LORA_DROPOUT,
        bias="none",
        use_gradient_checkpointing="unsloth",
        random_state=42,
        use_rslora=False,
        loftq_config=None,
    )
    print(f"âœ“ LoRA é…ç½®å®Œæˆ")
    print(f"  - Rank (r): {LORA_R}")
    print(f"  - Alpha: {LORA_ALPHA}")
    print(f"  - Dropout: {LORA_DROPOUT}")
    print(f"  - ç›®æ ‡æ¨¡å—: {TARGET_MODULES}")
    
    # æ‰“å°å¯è®­ç»ƒå‚æ•°
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"  - å¯è®­ç»ƒå‚æ•°: {trainable_params:,} / {total_params:,} "
          f"({100 * trainable_params / total_params:.2f}%)")
    
    # 3. åŠ è½½æ•°æ®é›†
    print(f"\n[3/5] åŠ è½½æ•°æ®é›†: {DATASET_NAME}...")
    dataset = load_dataset(
        DATASET_NAME,
        split=DATASET_SPLIT,
        streaming=True  # æµå¼åŠ è½½èŠ‚çœå†…å­˜
    )
    
    # é‡‡æ ·æŒ‡å®šæ•°é‡
    dataset = dataset.take(NUM_SAMPLES)
    
    print(f"âœ“ æ•°æ®é›†åŠ è½½å®Œæˆ")
    print(f"  - é‡‡æ ·æ•°é‡: {NUM_SAMPLES:,}")
    
    # 4. æ•°æ®é¢„å¤„ç†
    print("\n[4/5] å‡†å¤‡æ•°æ®é¢„å¤„ç†...")
    
    def formatting_func(examples):
        """æ ¼å¼åŒ–å‡½æ•°ï¼šå°†æ–‡æœ¬åŒ…è£…ä¸ºè®­ç»ƒæ ¼å¼"""
        texts = []
        for text in examples[TEXT_COLUMN]:
            # ç¡®ä¿æ–‡æœ¬ä¸ä¸ºç©º
            if text and len(text.strip()) > 0:
                # æ·»åŠ  EOS token
                formatted_text = text.strip() + tokenizer.eos_token
                texts.append(formatted_text)
        return texts
    
    print("âœ“ æ•°æ®é¢„å¤„ç†å‡½æ•°å‡†å¤‡å®Œæˆ")
    
    # 5. é…ç½®è®­ç»ƒå‚æ•°
    print("\n[5/5] é…ç½®è®­ç»ƒå‚æ•°...")
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
        warmup_steps=WARMUP_STEPS,
        num_train_epochs=NUM_TRAIN_EPOCHS,
        learning_rate=LEARNING_RATE,
        fp16=FP16,
        bf16=BF16,
        logging_steps=LOGGING_STEPS,
        optim=OPTIM,
        weight_decay=WEIGHT_DECAY,
        lr_scheduler_type="linear",
        seed=42,
        save_steps=SAVE_STEPS,
        save_total_limit=3,
        logging_dir=f"{OUTPUT_DIR}/logs",
        report_to="none",  # å¯æ”¹ä¸º "wandb" æˆ– "tensorboard"
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=dataset,
        dataset_text_field=TEXT_COLUMN,
        max_seq_length=MAX_SEQ_LENGTH,
        formatting_func=formatting_func,
        args=training_args,
        packing=False,  # Unsloth è‡ªåŠ¨ä¼˜åŒ–
    )
    
    print("âœ“ è®­ç»ƒå™¨é…ç½®å®Œæˆ")
    print(f"  - Batch size: {PER_DEVICE_TRAIN_BATCH_SIZE}")
    print(f"  - Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}")
    print(f"  - Effective batch size: {PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}")
    print(f"  - Learning rate: {LEARNING_RATE}")
    print(f"  - Epochs: {NUM_TRAIN_EPOCHS}")
    print(f"  - Optimizer: {OPTIM}")
    print(f"  - FP16: {FP16}, BF16: {BF16}")
    
    # å¼€å§‹è®­ç»ƒ
    print("\n" + "=" * 80)
    print("å¼€å§‹è®­ç»ƒ...")
    print("=" * 80 + "\n")
    
    gpu_stats = torch.cuda.get_device_properties(0)
    start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
    print(f"GPU: {gpu_stats.name}")
    print(f"GPU å†…å­˜: {start_gpu_memory} GB / {max_memory} GB")
    print()
    
    trainer_stats = trainer.train()
    
    # è®­ç»ƒå®Œæˆ
    print("\n" + "=" * 80)
    print("è®­ç»ƒå®Œæˆ!")
    print("=" * 80)
    
    used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
    used_memory_for_lora = round(used_memory - start_gpu_memory, 3)
    used_percentage = round(used_memory / max_memory * 100, 3)
    
    print(f"\nè®­ç»ƒç»Ÿè®¡:")
    print(f"  - å³°å€¼å†…å­˜ä½¿ç”¨: {used_memory} GB")
    print(f"  - LoRA é¢å¤–å†…å­˜: {used_memory_for_lora} GB")
    print(f"  - å†…å­˜ä½¿ç”¨ç‡: {used_percentage}%")
    
    # ä¿å­˜æ¨¡å‹
    print(f"\nä¿å­˜æ¨¡å‹åˆ°: {OUTPUT_DIR}")
    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    print("âœ“ æ¨¡å‹ä¿å­˜å®Œæˆ")
    
    # ä¿å­˜ä¸º GGUF æ ¼å¼ï¼ˆç”¨äºæ ‘è“æ´¾éƒ¨ç½²ï¼‰
    print("\nè½¬æ¢ä¸º GGUF æ ¼å¼...")
    try:
        model.save_pretrained_gguf(
            OUTPUT_DIR + "/gguf",
            tokenizer,
            quantization_method="q4_k_m"  # æ¨èç”¨äºæ ‘è“æ´¾
        )
        print("âœ“ GGUF æ¨¡å‹ä¿å­˜å®Œæˆ")
        print(f"  ä½ç½®: {OUTPUT_DIR}/gguf")
        print(f"  é‡åŒ–æ–¹æ³•: Q4_K_M (æ¨èç”¨äºæ ‘è“æ´¾)")
    except Exception as e:
        print(f"âš  GGUF è½¬æ¢å¤±è´¥: {e}")
        print("  å¯ä»¥ç¨åä½¿ç”¨ llama.cpp æ‰‹åŠ¨è½¬æ¢")
    
    print("\n" + "=" * 80)
    print("å…¨éƒ¨å®Œæˆ! ğŸ‰")
    print("=" * 80)
    print(f"\næ¨¡å‹ä¿å­˜ä½ç½®: {OUTPUT_DIR}")
    print("\nä¸‹ä¸€æ­¥:")
    print("1. åœ¨æ ‘è“æ´¾ä¸Šå®‰è£… llama.cpp æˆ– Ollama")
    print("2. å°† GGUF æ¨¡å‹ä¼ è¾“åˆ°æ ‘è“æ´¾")
    print("3. è¿è¡Œæ¨ç†æµ‹è¯•")


if __name__ == "__main__":
    main()